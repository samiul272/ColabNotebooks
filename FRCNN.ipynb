{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FRCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiul272/ColabNotebooks/blob/master/FRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ViR2n2rXlTB",
        "colab_type": "code",
        "outputId": "fd8ea394-4ac3-4755-b27c-c95494be252e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import traceback\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from keras.layers import Conv2D\n",
        "from keras.models import Input, Model\n",
        "from keras.applications import InceptionResNetV2\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from utils import generate_anchors, draw_anchors, bbox_overlaps, bbox_transform,\\\n",
        "                    loss_cls, smoothL1, parse_label, unmap"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1e3950cbf8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInceptionResNetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_overlaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_transform\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mloss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNbtiaBoE06Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 9\n",
        "feature_map_tile = Input(shape=(None,None,1536))\n",
        "\n",
        "convolution_3x3 = Conv2D(\n",
        "    filters=512,\n",
        "    kernel_size=(3, 3),\n",
        "    name=\"3x3\"\n",
        ")(feature_map_tile)\n",
        "\n",
        "output_deltas = Conv2D(\n",
        "    filters= 4 * k,\n",
        "    kernel_size=(1, 1),\n",
        "    activation=\"linear\",\n",
        "    kernel_initializer=\"uniform\",\n",
        "    name=\"deltas1\"\n",
        ")(convolution_3x3)\n",
        "\n",
        "output_scores = Conv2D(\n",
        "    filters=1 * k,\n",
        "    kernel_size=(1, 1),\n",
        "    activation=\"sigmoid\",\n",
        "    kernel_initializer=\"uniform\",\n",
        "    name=\"scores1\"\n",
        ")(convolution_3x3)\n",
        "\n",
        "rpn_model = Model(inputs=[feature_map_tile], outputs=[output_scores, output_deltas])\n",
        "rpn_model.compile(optimizer='adam', loss={'scores1':loss_cls, 'deltas1':smoothL1})\n",
        "rpn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYf38AjvcpHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -O http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPbIVUnsix4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tarfile import TarFile\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "\n",
        "with TarFile('VOCtrainval_11-May-2012.tar') as tar:\n",
        "    for f in tqdm_notebook(tar.getnames()):\n",
        "        tar.extract(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jllghsKCrHNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "os.listdir('./VOCdevkit/VOC2012/JPEGImages/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JINEQPzEwpt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category, gt_boxes, scale = parse_label(anno_path+'2011_006777.xml')\n",
        "print(category, gt_boxes, scale)\n",
        "produce_batch(img_path+'2011_006777.jpg', gt_boxes, scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22wRoryh3JQ8",
        "colab_type": "code",
        "outputId": "1d84bc49-04c0-4577-bbaf-e6ccf12a609b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "int(('281.70000076293945'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c37df33ce496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'281.70000076293945'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '281.70000076293945'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhrMBRJQqdf1",
        "colab_type": "code",
        "outputId": "4805e7ac-4f2f-4768-ca09-d83039ba4f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "##################  prepare batch  #######################\n",
        "BG_FG_FRAC=2\n",
        "\n",
        "#load an example to void graph problem\n",
        "#TODO fix this.\n",
        "pretrained_model = InceptionResNetV2(include_top=False)\n",
        "img=load_img(\"./VOCdevkit/VOC2012/JPEGImages/2007_000027.jpg\")\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "not_used=pretrained_model.predict(x)\n",
        "\n",
        "def produce_batch(filepath, gt_boxes, scale):\n",
        "    img=load_img(filepath)\n",
        "    img_width=np.shape(img)[1] * scale[1]\n",
        "    img_height=np.shape(img)[0] * scale[0]\n",
        "    img=img.resize((int(img_width),int(img_height)))\n",
        "    #feed image to pretrained model and get feature map\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    feature_map=pretrained_model.predict(img)\n",
        "    height = np.shape(feature_map)[1]\n",
        "    width = np.shape(feature_map)[2]\n",
        "    num_feature_map=width*height\n",
        "    #calculate output w, h stride\n",
        "    w_stride = img_width / width\n",
        "    h_stride = img_height / height\n",
        "    #generate base anchors according output stride.\n",
        "    #base anchors are 9 anchors wrt a tile (0,0,w_stride-1,h_stride-1)\n",
        "    base_anchors=generate_anchors(w_stride,h_stride)\n",
        "    #slice tiles according to image size and stride.\n",
        "    #each 1x1x1532 feature map is mapping to a tile.\n",
        "    shift_x = np.arange(0, width) * w_stride\n",
        "    shift_y = np.arange(0, height) * h_stride\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
        "                            shift_y.ravel())).transpose()\n",
        "    #apply base anchors to all tiles, to have a num_feature_map*9 anchors.\n",
        "    all_anchors = (base_anchors.reshape((1, 9, 4)) +\n",
        "                    shifts.reshape((1, num_feature_map, 4)).transpose((1, 0, 2)))\n",
        "    total_anchors = num_feature_map*9\n",
        "    all_anchors = all_anchors.reshape((total_anchors, 4))\n",
        "    #only keep anchors inside image+borader.\n",
        "    border=0\n",
        "    inds_inside = np.where(\n",
        "            (all_anchors[:, 0] >= -border) &\n",
        "            (all_anchors[:, 1] >= -border) &\n",
        "            (all_anchors[:, 2] < img_width+border ) &  # width\n",
        "            (all_anchors[:, 3] < img_height+border)    # height\n",
        "    )[0]\n",
        "    anchors=all_anchors[inds_inside]\n",
        "    # calculate overlaps each anchors to each gt boxes,\n",
        "    # a matrix with shape [len(anchors) x len(gt_boxes)]\n",
        "    overlaps = bbox_overlaps(anchors, gt_boxes)\n",
        "    # find the gt box with biggest overlap to each anchors,\n",
        "    # and the overlap ratio. result (len(anchors),)\n",
        "    argmax_overlaps = overlaps.argmax(axis=1)\n",
        "    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n",
        "    # find the anchor with biggest overlap to each gt boxes,\n",
        "    # and the overlap ratio. result (len(gt_boxes),)\n",
        "    gt_argmax_overlaps = overlaps.argmax(axis=0)\n",
        "    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n",
        "                                np.arange(overlaps.shape[1])]\n",
        "    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n",
        "    #labels, 1=fg/0=bg/-1=ignore\n",
        "    labels = np.empty((len(inds_inside), ), dtype=np.float32)\n",
        "    labels.fill(-1)\n",
        "    # set positive label, define in Paper3.1.2:\n",
        "    # We assign a positive label to two kinds of anchors: (i) the\n",
        "    # anchor/anchors with the highest Intersection-overUnion\n",
        "    # (IoU) overlap with a ground-truth box, or (ii) an\n",
        "    # anchor that has an IoU overlap higher than 0.7 with any gt boxes\n",
        "    labels[gt_argmax_overlaps] = 1\n",
        "    labels[max_overlaps >= .7] = 1\n",
        "    # set negative labels\n",
        "    labels[max_overlaps <= .3] = 0\n",
        "    # subsample positive labels if we have too many\n",
        "#     num_fg = int(RPN_FG_FRACTION * RPN_BATCHSIZE)\n",
        "    fg_inds = np.where(labels == 1)[0]\n",
        "#     if len(fg_inds) > num_fg:\n",
        "#         disable_inds = npr.choice(\n",
        "#             fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n",
        "#         labels[disable_inds] = -1\n",
        "    # subsample negative labels if we have too many\n",
        "    num_bg = int(len(fg_inds) * BG_FG_FRAC)\n",
        "    bg_inds = np.where(labels == 0)[0]\n",
        "    if len(bg_inds) > num_bg:\n",
        "        disable_inds = npr.choice(\n",
        "            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n",
        "        labels[disable_inds] = -1\n",
        "    #\n",
        "    batch_inds=inds_inside[labels!=-1]\n",
        "    batch_inds=(batch_inds / k).astype(np.int)\n",
        "    full_labels = unmap(labels, total_anchors, inds_inside, fill=-1)\n",
        "    batch_label_targets=full_labels.reshape(-1,1,1,1*k)[batch_inds]\n",
        "    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n",
        "    # bbox_targets = bbox_transform(anchors, gt_boxes[argmax_overlaps, :]\n",
        "    pos_anchors=all_anchors[inds_inside[labels==1]]\n",
        "    bbox_targets = bbox_transform(pos_anchors, gt_boxes[argmax_overlaps, :][labels==1])\n",
        "    bbox_targets = unmap(bbox_targets, total_anchors, inds_inside[labels==1], fill=0)\n",
        "    batch_bbox_targets = bbox_targets.reshape(-1,1,1,4*k)[batch_inds]\n",
        "    padded_fcmap=np.pad(feature_map,((0,0),(1,1),(1,1),(0,0)),mode='constant')\n",
        "    padded_fcmap=np.squeeze(padded_fcmap)\n",
        "    batch_tiles=[]\n",
        "    for ind in batch_inds:\n",
        "        x = ind % width\n",
        "        y = int(ind/width)\n",
        "        fc_3x3=padded_fcmap[y:y+3,x:x+3,:]\n",
        "        batch_tiles.append(fc_3x3)\n",
        "    return np.asarray(batch_tiles), batch_label_targets.tolist(), batch_bbox_targets.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 07:26:40.785135 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0823 07:26:40.840092 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0823 07:26:40.850423 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0823 07:26:40.889582 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0823 07:26:40.891148 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0823 07:26:43.977457 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0823 07:26:44.322356 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0823 07:26:45.067847 139685507786624 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 15s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05bd796f778e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#TODO fix this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionResNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./VOCdevkit/VOC2012/JPEGImages/2007_000027.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './VOCdevkit/VOC2012/JPEGImages/2007_000027.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4_fTIsUaPQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################  generate data  #######################\n",
        "ILSVRC_dataset_path='./VOCdevkit/VOC2012/'\n",
        "img_path=ILSVRC_dataset_path+'JPEGImages/'\n",
        "anno_path=ILSVRC_dataset_path+'Annotations/'\n",
        "import glob\n",
        "\n",
        "BATCH_SIZE=4\n",
        "def input_generator():\n",
        "    batch_tiles=[]\n",
        "    batch_labels=[]\n",
        "    batch_bboxes=[]\n",
        "    count=0\n",
        "    while 1:\n",
        "        for fname in glob.glob('./VOCdevkit/VOC2012/ImageSets/*/*train.txt'):\n",
        "            with open(fname,'r') as f:\n",
        "                for line in f:\n",
        "                    if 'extra' not in line:\n",
        "                        try:\n",
        "                            category, gt_boxes, scale = parse_label(anno_path+line.split()[0]+'.xml')\n",
        "                            if len(gt_boxes)==0:\n",
        "                                continue\n",
        "                            tiles, labels, bboxes = produce_batch(img_path+line.split()[0]+'.jpg', gt_boxes, scale)\n",
        "                        except Exception:\n",
        "                            print('parse label or produce batch failed: for: '+line.split()[0])\n",
        "                            print(fname)\n",
        "                            break\n",
        "                        for i in range(len(tiles)):\n",
        "                            batch_tiles.append(tiles[i])\n",
        "                            batch_labels.append(labels[i])\n",
        "                            batch_bboxes.append(bboxes[i])\n",
        "                            if(len(batch_tiles)==BATCH_SIZE):\n",
        "                                a=np.asarray(batch_tiles)\n",
        "                                b=np.asarray(batch_labels)\n",
        "                                c=np.asarray(batch_bboxes)\n",
        "                                if not a.any() or not b.any() or not c.any():\n",
        "                                    print(\"empty array found.\")\n",
        "\n",
        "                                yield a, [b, c]\n",
        "                                batch_tiles=[]\n",
        "                                batch_labels=[]\n",
        "                                batch_bboxes=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv2s_fw4kSxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='./weights.hdf5', verbose=1, save_best_only=True)\n",
        "rpn_model.fit_generator(input_generator(), steps_per_epoch=1000, epochs=800, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFQTzD5OURDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x, y = input_generator().__next__()\n",
        "print(x[0,:,:,100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JTx8GMXtik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}